\section{Introduction \& the idea}
DeepProbLog \cite{DBLP} can be used to incorporate logical predicates into a neural network model. Here, we try to compare a DeepProbLog model with a normal CNN by training it on the simple game rock paper scissors. Testing if the DeepProbLog model has an advantage over the normal CNN is the goal of this experiment. By increasing the complexity of the game, which is done by adding 2 possible gestures lizard and spock, the effect on the performance of both models can be compared. We want to see what happens when the complexity increases, how both models handle it and if the DeepProbLog model is more scalable. We believe that by incorporating the logical predicate we give the model additional information of rules which will result in a faster learning process compared to a CNN that does not have any understanding of these rules. The results are interesting to see how big of a difference these logical models have over the default CNN, it can become a useful technique to optimize existing models.
